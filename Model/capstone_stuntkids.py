# -*- coding: utf-8 -*-
"""Capstone_StuntKids.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HFElwowrchYmRP0gXT-Zx46eeKaUb59h
"""

!pip install keras_tuner

"""# Import Library and Packages"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import joblib
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
import keras_tuner as kt

"""# Data Preparation

Data is taken from: https://www.kaggle.com/datasets/rendiputra/stunting-balita-detection-121k-rows

## Gathering Data
"""

data = pd.read_csv('data_balita.csv')
data.head()

"""This table contains 4 columns. Below is an explanation of these 4 columns:

*   Umur (bulan):  The child's age in months
*   Jenis Kelamin : Male or female
*   Tinggi Badan (cm) : The child's height in centimeters.
*   Status Gizi :

      *    Normal
      *   Severely Stunted
      *   Stunted
      *   Tinggi

## Assessing Data
"""

data.info()

"""There are no inconsistencies in the data."""

print("Jumlah duplikasi: ", data.duplicated().sum())
data.describe()

"""There are 81574 duplicated rows.

"""

data.isna().sum()

"""There are no columns with missing values."""

data.dtypes

"""To display the data type of each column:


*   Umur (bulan): Represented as an integer
*   Jenis Kelamin: Represented as a string or categorical
*   Tinggi Badan (cm): Represented as a floating-point number
*   Status Gizi: Represented as a string or categorical


"""

data[data.duplicated(subset=['Jenis Kelamin', 'Umur (bulan)', 'Tinggi Badan (cm)','Status Gizi'])]

"""Display the duplicate rows based on the values in the columns of jenis kelamin, umur, tinggi badan, dan status gizi."""

data[~data.duplicated(subset=['Jenis Kelamin', 'Umur (bulan)', 'Tinggi Badan (cm)', 'Status Gizi'])]

"""Filter the unique rows, the non-duplicate rows based on the values in the columns of jenis kelamin, umur, tinggi badan, dan status gizi.

## Cleaning Data
"""

data = data.drop_duplicates()
print(f"Banyaknya data setelah proses cleaning: {len(data)}")

"""Delete the duplicate rows, which total 39425.

## Features Engineering

**Categorical Encoding:**
"""

le = LabelEncoder()
data['Jenis Kelamin'] = le.fit_transform(data['Jenis Kelamin'])
print("Kategori Jenis Kelamin yang telah dienkode secara berurutan [0,1]: ", le.classes_)
data['Status Gizi'] = le.fit_transform(data['Status Gizi'])
print("Kategori Status Gizi yang telah dienkode secara berurutan [0,1,2,3]: ", le.classes_)
joblib.dump(le, 'label_encoder.pkl')

"""To convert categorical columns into numeric values (integers)"""

data['Status Gizi'].value_counts().sort_index()

"""Calculate the frequency of each category in the **Status Gizi** column, and then sort them by index (category).

It appears that the most common data status is "Normal," while the least common is "Stunted."
"""

data.to_csv('data_cleaned.csv', index=False)

"""Save the cleaned dataset."""

x = data.drop(columns=['Status Gizi'])
y = data['Status Gizi']

"""x will store a DataFrame containing the features that will be used for prediction. This DataFrame will include all columns except for Status Gizi.

y will store the Status Gizi column, which is the target for the model.

**SMOTE:**

SMOTE (Synthetic Minority Over-sampling Technique) is a technique used to perform oversampling on imbalanced datasets.
"""

smote = SMOTE(random_state=42)
x_resampled, y_resampled = smote.fit_resample(x, y)

data_balanced = pd.concat([pd.DataFrame(x_resampled, columns=x.columns), pd.Series(y_resampled, name='Status Gizi')], axis=1)
print("Banyaknya isi setiap kelas setelah oversampling:")
print(data_balanced['Status Gizi'].value_counts())

"""Before applying SMOTE, the data in each category of the **Status Gizi** column was imbalanced. After performing SMOTE, the number of data points in each category is now equal."""

data_balanced.to_csv('data_after_oversampling.csv', index=False)

"""Save after the oversampling dataset."""

x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)

"""The oversampled dataset will be split into training data (train) and testing data (test).

20% of the data will be used for **x_test** and **y_test**, while 80% will be used for **x_train** and **y_train** (selected randomly).

**NORMALIZATION and SCALING:**
"""

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
joblib.dump(scaler, 'scaler.pkl')

"""Perform standardization or normalization on the data to ensure that the features have the same scale."""

print("Shape x_train:", x_train.shape)

"""Display the dimensions of **x_train**, which refers to the number of rows and columns after the data splitting and standardization process.

#Model

## Building a Model Using TensorFlow
"""

model = Sequential([
    Input(shape=(x_train.shape[1],)),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(4, activation='softmax')
])

"""Apply a model to the data

**Compile Model:**
"""

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""**Train the model using the fit() method:**"""

history = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_data=(x_test, y_test))

"""After training, the model achieved an accuracy of 95% with a loss of 0.1."""

model.summary()

# Plot loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

# Plot akurasi
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.show()

"""Visualize the model's performance by displaying the loss and accuracy graphs for the training and validation data."""

y_pred = (model.predict(x_test) > 0.5).astype("int32")

"""Predicting target data with a trained model

**Evaluates model:**
"""

y_pred_classes = np.argmax(y_pred, axis=1)
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

"""It appears that a classification performance report is generated, including:

* **Precision**: The proportion of positive predictions that are correct.
* **Recall**: The proportion of actual positive data correctly identified.
* **F1-Score**: The harmonic mean of precision and recall.
* **Support**: The number of samples for each class.

**Display the Confusion Matrix:**
"""

cm = confusion_matrix(y_test, y_pred_classes)

cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
cmd.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""Based on this confusion matrix, it shows the number of correct and incorrect predictions for each class.

## Hyperparameter Tuning
"""

# Definisikan fungsi untuk membangun model dengan hyperparameter tuning
def build_model(hp):
    model = Sequential()
    # Pilih jumlah neuron pada layer pertama (64, 128, 256)
    model.add(Input(shape=(x_train.shape[1],)))
    model.add(Dense(units=hp.Int('units_input', min_value=64, max_value=256, step=64),
                    activation='relu'))
    model.add(Dropout(rate=hp.Float('dropout_input', min_value=0.2, max_value=0.5, step=0.1)))

    # Tambahkan layer tersembunyi yang dapat di-tuning
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=128, step=32),
                        activation='relu'))
        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))

    # Lapisan output
    model.add(Dense(4, activation='softmax'))

    # Pilih learning rate dari optimizer
    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Atur tuner untuk mencari parameter terbaik
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,  # Jumlah kombinasi hyperparameter yang ingin dicoba
    executions_per_trial=2,  # Melakukan beberapa eksekusi per trial untuk hasil yang lebih konsisten
    directory='keras_tuner_dir',
    project_name='tuning_gizi_model'
)

# Jalankan tuner untuk mencari parameter terbaik
tuner.search(x_train, y_train, epochs=15, validation_data=(x_test, y_test))

# Ambil model terbaik
best_model = tuner.get_best_models(num_models=1)[0]

# Evaluasi model terbaik
val_loss, val_accuracy = best_model.evaluate(x_test, y_test)
print(f"Best model validation accuracy: {val_accuracy}")

# Print hyperparameter terbaik
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters:")
print(f"Units per layer: {best_hyperparameters.get('units_input')}")
print(f"Learning rate: {best_hyperparameters.get('learning_rate')}")
for i in range(best_hyperparameters.get('num_layers')):
    print(f"Units in layer {i+1}: {best_hyperparameters.get(f'units_{i}')}")
    print(f"Dropout rate in layer {i+1}: {best_hyperparameters.get(f'dropout_{i}')}")

"""The results obtained from this hyperparameter tuning process successfully improved the model's accuracy. The validation accuracy reached 97%, and the training accuracy reached 98%."""

best_model.summary()

y_pred = best_model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)

"""**Evaluates Best Model:**"""

print("Classification Report:")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

"""It can be seen that the classification performance report has improved from before.

**Display the Confusion Matrix:**
"""

cm = confusion_matrix(y_test, y_pred_classes)
cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
cmd.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""It can be seen that the number of correct predictions increases for each class increase.

#Save Model
"""

model.save('model.h5')
best_model.save('best_model.h5')

model.save_weights('model.weights.h5')
best_model.save_weights('model.weights.h5')

model.save('model.keras')
best_model.save('best_model.keras')

"""## Convert Model to TFLite"""

model_converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_tflite = model_converter.convert()

with open('/content/model.tflite', 'wb') as f:
    f.write(model_tflite)

best_model_converter = tf.lite.TFLiteConverter.from_keras_model(best_model)
best_model_tflite = best_model_converter.convert()

with open('/content/best_model.tflite', 'wb') as f:
    f.write(best_model_tflite)

print("Models saved as .tflite")

# Path ke model TFLite
tflite_model_path = '/content/best_model.tflite'

# Load model TFLite
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Dapatkan informasi tentang input dan output tensor
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Periksa shape dan tipe input/output
print(f"Input details: {input_details}")
print(f"Output details: {output_details}")

# Contoh input data (ubah sesuai dengan data yang kamu punya)
input_data = np.array(np.random.random_sample(input_details[0]['shape']), dtype=np.float32)

# Set input tensor
interpreter.set_tensor(input_details[0]['index'], input_data)

# Jalankan inferensi
interpreter.invoke()

# Ambil hasil output
output_data = interpreter.get_tensor(output_details[0]['index'])
print("Output:", output_data)

from tensorflow.keras.models import load_model

savedModel=load_model('best_model.h5')
savedModel.summary()

a = load_model('model.h5')
b = load_model('best_model.h5')
val_loss, val_accuracy = a.evaluate(x_test, y_test)
print(f"Model Validation Accuracy: {val_accuracy}")

val_loss_best, val_accuracy_best = b.evaluate(x_test, y_test)
print(f"Best Model Validation Accuracy: {val_accuracy_best}")

"""# Prediction and Model Testing

Try loading the model and making predictions by inputting data such as age, height, and gender. Based on the prediction results, the script provides food recommendations that are suitable for the detected nutritional status.
"""

# Memuat model yang telah dilatih
model = tf.keras.models.load_model('best_model.h5')


# Data input (contoh)
umur = 38
tinggi_badan = 109.8
jenis_kelamin = 0  # 0 untuk laki-laki, 1 untuk perempuan

# Proses input menjadi bentuk yang sesuai untuk model
data_input = np.array([umur, jenis_kelamin, tinggi_badan]).reshape(1, -1)
print((data_input))
data_input = scaler.transform(data_input)
print((data_input))

# Melakukan prediksi
y_pred = model.predict(data_input)
hasil_prediksi = np.argmax(y_pred, axis=1)
print((y_pred))

# Mengubah hasil prediksi menjadi nama kelas
class_names = le.inverse_transform(hasil_prediksi)
hasil_prediksi = class_names[0]


# Percabangan untuk rekomendasi berdasarkan hasil prediksi dan umur
if hasil_prediksi == 'normal':
    if umur <= 6:
        frekuensi_makan = "Sesuai kebutuhan bayi"
        rekomendasi_makanan = "ASI Eksklusif"
    elif 6 < umur < 12:
        frekuensi_makan = "2-3 kali MPASI, 1-2 Makanan Selingan + ASI"
        rekomendasi_makanan = {
            'Protein 30-45 gr': ['ikan', 'daging', 'ayam', 'telur 1/2 butir'],
            'Lemak 30 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 125 ml:': ['ubi', 'kentang', 'nasi'],
            'Sayur 100 gr': ['bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 50 gr:': ['pisang', 'jeruk', 'kiwi'],
            'Susu 250 ml': ['ASI', 'Susu Formula Pertumbuhan']
        }
    else:
        frekuensi_makan = "3 Makanan Utama, 1 Makanan Selingan"
        rekomendasi_makanan = {
            'Protein 26 gr': ['ikan', 'daging', 'ayam', 'telur 1 butir'],
            'Lemak 35-40 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 150 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 200 gr': ['bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 200 gr': ['pisang', 'jeruk', 'kiwi'],
            'Susu 250 ml' : ['Susu Formula Pertumbuhan', 'Susu Sapi UHT', 'Susu Kedelai']
        }

    print(f"Anak terdeteksi Normal. Berikut beberapa informasi yang bisa berguna:")
    print("Silakan cek artikel mengenai stunting di bawah ini untuk informasi lebih lanjut:")
    print("1. https://humbanghasundutankab.go.id/main/index.php/read/news/828")
    print("2. https://stunting.go.id/")
    print("3. https://berkas.dpr.go.id/pusaka/files/info_singkat/Info%20Singkat-XV-14-II-P3DI-Juli-2023-196.pdf")


elif hasil_prediksi == 'severely stunted':
    if umur <= 6:
        frekuensi_makan = "Sesuai kebutuhan bayi"
        rekomendasi_makanan = "ASI Eksklusif"
    elif 6 < umur < 12:
        frekuensi_makan = "3-4 kali MPASI, 1-2 Makanan Selingan + ASI"
        rekomendasi_makanan = {
            'Protein 45-60 gr': ['ikan', 'daging', 'ayam', 'telur 1 butir'],
            'Lemak 40 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 125 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 100 gr': ['bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 50-100 gr:': ['pisang', 'jeruk', 'kiwi'],
            'Susu 300-350 ml': ['ASI', 'Susu Formula Pertumbuhan']
        }
    else:
        frekuensi_makan = "3 Makanan Utama, 2 Makanan Selingan"
        rekomendasi_makanan = {
            'Protein 26-30 gr': ['ikan', 'daging', 'ayam', 'telur 1 butir'],
            'Lemak 40 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 150-160 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 200 gr': ['bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 200 gr': ['pisang', 'jeruk', 'kiwi'],
            'Susu 350 ml' : ['Susu Formula Pertumbuhan', 'Susu Sapi UHT', 'Susu Kedelai']
        }
    print("Anak terdeteksi Severely Stunting. Mohon segera periksakan anak ke ahli gizi atau puskesmas terdekat.")

elif hasil_prediksi == 'stunted':
    if umur <= 6:
        frekuensi_makan = "Sesuai kebutuhan bayi"
        rekomendasi_makanan = "ASI Eksklusif"
    elif 6 < umur < 12:
        frekuensi_makan = "3 kali MPASI, 1-2 Makanan Selingan + ASI"
        rekomendasi_makanan = {
            'Protein 45 gr': ['ikan', 'daging', 'ayam', 'telur 1/2 - 1 butir'],
            'Lemak 30-40 gr': [ 'minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 125 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 100 gr': [ 'bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 50 gr': ['pisang', 'jeruk', 'kiwi'],
            'Susu 250 ml': ['ASI', 'Susu Formula Pertumbuhan']
        }
    else:
        frekuensi_makan = "3 Makanan Utama, 1-2 Makanan Selingan"
        rekomendasi_makanan = {
            'Protein 20-26 gr': ['ikan', 'daging', 'ayam', 'telur 1 butir'],
            'Lemak 35-40 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 150 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 200 gr': [ 'bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 200 gr': [ 'pisang', 'jeruk', 'kiwi'],
            'Susu 250-300 ml': ['Susu Formula Pertumbuhan', 'Susu Sapi UHT', 'Susu Kedelai']
        }

    print("Anak terdeteksi Stunting. Mohon segera periksakan anak ke ahli gizi atau puskesmas terdekat.")

elif hasil_prediksi == 'tinggi':
    if umur <= 6:
        frekuensi_makan = "Sesuai kebutuhan bayi"
        rekomendasi_makanan = "ASI Eksklusif"
    elif 6 < umur < 12:
        frekuensi_makan = "3 kali MPASI, 1-2 Makanan Selingan + ASI"
        rekomendasi_makanan = {
            'Protein 60 gr': ['ikan', 'daging', 'ayam', 'telur 1/2 - 1 butir'],
            'Lemak 40-45 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 150 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 100 gr': ['bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 200 gr': ['pisang', 'jeruk', 'kiwi'],
            'Susu 300-350 ml': ['ASI', 'Susu Formula Pertumbuhan']
        }
    else:
        frekuensi_makan = "3 Makanan Utama, 2 Makanan Selingan"
        rekomendasi_makanan = {
            'Protein 30-35 gr': ['ikan', 'daging', 'ayam', 'telur 1 butir'],
            'Lemak 40-45 gr': ['minyak Sehat (minyak ikan atau minyak kedelai)'],
            'Karbohidrat 160-180 ml': ['ubi', 'kentang', 'nasi'],
            'Sayur 200 gr': ['bayam', 'labu kuning', 'brokoli', 'wortel'],
            'Buah 200 gr': ['pisang', 'jeruk', 'kiwi'],
            'Susu 350 ml' : ['Susu Formula Pertumbuhan', 'Susu Sapi UHT', 'Susu Kedelai']
        }

    print(f"Anak terdeteksi memiliki pertumbuhan yang Tinggi. Berikut beberapa informasi yang bisa berguna:")
    print("Silakan cek artikel mengenai stunting di bawah ini untuk informasi lebih lanjut:")
    print("1. https://humbanghasundutankab.go.id/main/index.php/read/news/828")
    print("2. https://stunting.go.id/")
    print("3. https://berkas.dpr.go.id/pusaka/files/info_singkat/Info%20Singkat-XV-14-II-P3DI-Juli-2023-196.pdf")



# Menampilkan rekomendasi
if isinstance(rekomendasi_makanan, dict):
    print(f"Frekuensi Makan: {frekuensi_makan}")
    print("Rekomendasi Makanan:")
    for kategori, items in rekomendasi_makanan.items():
        print(f"\n{kategori}:")
        for item in items:
            print(f"  - {item}")
else:
    # Jika rekomendasi_makanan bukan dictionary, hanya tampilkan sebagai string
    print(f"Frekuensi Makan: {frekuensi_makan}")
    print(f"Rekomendasi Makanan: {rekomendasi_makanan}")